{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNJHjkCWI7ZmLlBlpbFcPYC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dewangulbuddin/machine-learning-iitm/blob/main/Week_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Week: 1"
      ],
      "metadata": {
        "id": "W8GaLHuosnt6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 1: Look at the big picture"
      ],
      "metadata": {
        "id": "ZxmU1gxrlPL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Frame the problem\n",
        "2. Select performance measure\n",
        "3. List and check assumptions\n",
        "\n",
        "  1.1 Frame the problem\n",
        "\n",
        "      * What is input and output?\n",
        "      * Business objectives from the model\n",
        "      * Supervised, unsupervised or RL problem\n",
        "      * Classification, regression or some other task\n",
        "      * Single or multiple outputs\n",
        "      * Continuous learning or periodic updates\n",
        "      * Batch or online learning\n",
        "  \n",
        "  1.2 Selection of performance measure\n",
        "\n",
        "      * Regression\n",
        "        * Mean squared error (MSE) or,\n",
        "        * Mean Absolute error (MAE)\n",
        "\n",
        "      * Classification metric\n",
        "        * Precision\n",
        "        * Recall\n",
        "        * F1-Score\n",
        "        * Accuracy\n"
      ],
      "metadata": {
        "id": "LO4ZH-MSiK1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2: Get the data"
      ],
      "metadata": {
        "id": "D222eVmEh_as"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5EVWQ8nr9O_C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt     #for graphing purpose\n",
        "import seaborn as sns               #for plotting histogram\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
        "#we use pandas to read the data from the csv file\n",
        "data = pd.read_csv(data_url, sep = \";\") #specify the separator by identifying it in the csv file\n",
        "#Here since we use pd.read_csv, the results stored in \"data\" will already be\n",
        "#in dataframe format"
      ],
      "metadata": {
        "id": "kDygIYNi98Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the first 5 rows\n",
        "data.head()"
      ],
      "metadata": {
        "id": "kD4xEhEb-f5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#separate features and labels\n",
        "#apart from the last column [:-1], all are features\n",
        "feature_list = data.columns[:-1].values\n",
        "#print(\"Feature List: \",feature_list)\n",
        "#last column is the label\n",
        "label = [data.columns[-1]]\n",
        "#print(\"Label: \",label)"
      ],
      "metadata": {
        "id": "yFiPvOmo-ilt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Statistics"
      ],
      "metadata": {
        "id": "LFa4jCGClY0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#basic info on data\n",
        "data.info()"
      ],
      "metadata": {
        "id": "Ct7rBhCE--bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#learn about the numeric attributes of the data\n",
        "data.describe()"
      ],
      "metadata": {
        "id": "CASiMowk_wNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#count the number of samples under a given quality index\n",
        "data['quality'].value_counts()"
      ],
      "metadata": {
        "id": "yBowuqNWENdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualise the above count\n",
        "sns.set()\n",
        "data.quality.hist()\n",
        "plt.xlabel('Wine Quality')\n",
        "plt.ylabel('Count')"
      ],
      "metadata": {
        "id": "TGRrQ_w6EofT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Make Test and Training Set"
      ],
      "metadata": {
        "id": "W2RSKyppljeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#make a function to split the training and testing data\n",
        "def split_train_test(data,test_ratio):\n",
        "  np.random.seed(42)                                        #setting random.seed(n), will allow us to get same random number for every test set\n",
        "  shuffled_indices = np.random.permutation(len(data))       #random.permutation(x) will randomly shuffle the numbers, 0 to (x-1)\n",
        "  test_set_size = int(len(data) * test_ratio)                          \n",
        "  test_indices = shuffled_indices[:test_set_size]               #generate test set indices\n",
        "  train_indices = shuffled_indices[test_set_size:]              #generate training dataset indices\n",
        "  return data.iloc[train_indices], data.iloc[test_indices]\n",
        "#call the above function & specify the splitting ratio\n",
        "  train_set_m, test_set_m = split_train_test(data,0.2)          #split as 80-20, m for manual"
      ],
      "metadata": {
        "id": "wtRSCiu2FATX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#or make test sets using sci-kit\n",
        "#here we use Random Sampling\n",
        "#Random Sampling randomly selects k% points for the test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_set_r, test_set_r = train_test_split(data, test_size = 0.2, random_state = 42)\n",
        "#specify the random_state so that we get same sets everytime we run this piece\n",
        "#of code, so as to get consistent result for study purpose\n",
        "#during actual generation, we can leave the random_state at default"
      ],
      "metadata": {
        "id": "QPPcUj3FcoI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#here we use Stratified Sampling\n",
        "#SSS divides samples such that they are representative of overall distribution\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n",
        "#n_splits is the no of re-shuffling and splitting iterations\n",
        "for train_index, test_index in sss.split(data, data[\"quality\"]):\n",
        "#.split() is a method of sss, which generates indices to split a data into\n",
        "#training and test set\n",
        "#use loc to select the data based on the index, specified in the argument of the method\n",
        "  train_set_s = data.loc[train_index]\n",
        "  test_set_s = data.loc[test_index]"
      ],
      "metadata": {
        "id": "DsH2L-EoOJKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Sampling Bias Comparision"
      ],
      "metadata": {
        "id": "TCsiWdsOQ5_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing it for comparision in terms of percentages:\n",
        "overall_dist = data[\"quality\"].value_counts()/len(data)\n",
        "random_dist = test_set_r[\"quality\"].value_counts()/len(test_set_r)\n",
        "strat_dist = test_set_s[\"quality\"].value_counts()/len(test_set_s)\n",
        "#lesser the difference better the distribution achieved"
      ],
      "metadata": {
        "id": "eWf8WWZfT3LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dist_comp = pd.DataFrame({'overall':overall_dist, 'stratified': strat_dist, 'random': random_dist})\n",
        "dist_comp['s-o'] = dist_comp['stratified'] - dist_comp['overall']\n",
        "dist_comp['(s-o)%'] = 100*dist_comp['s-o']/dist_comp['overall']\n",
        "dist_comp['r-o'] = dist_comp['random'] - dist_comp['overall']\n",
        "dist_comp['(r-o)%'] = 100*dist_comp['r-o']/dist_comp['overall']\n",
        "dist_comp"
      ],
      "metadata": {
        "id": "0jw5xjy9DGvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 3: Data Visualisation"
      ],
      "metadata": {
        "id": "WQm3-vT1OS6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#explore using stratfied sampling\n",
        "exploration_set = train_set_s.copy()\n",
        "#copy the training set to exploration set, to prevent change in original data \n",
        "#incase of modification during exploration. Here we use the complete set because\n",
        "#of small training data\n",
        "\n",
        "#using seaborn library\n",
        "sns.scatterplot(x = 'fixed acidity', y = 'density', hue = 'quality', data = exploration_set)"
      ],
      "metadata": {
        "id": "5x59XAL6ONMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using matplotlib\n",
        "exploration_set.plot(kind = 'scatter', x = 'fixed acidity', y = 'density', \n",
        "                     alpha = 0.5, c = 'quality', cmap = plt.get_cmap('jet'))"
      ],
      "metadata": {
        "id": "LjVAWd56keYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate correlation between features\n",
        "corr_matrix = exploration_set.corr()\n",
        "\n",
        "#lets check features that are correlated with the label, here 'quality'\n",
        "corr_matrix['quality']\n",
        "#+1 = strong postitve correlation\n",
        "#-1 = strong negative correlation\n",
        "#0  = no correlation"
      ],
      "metadata": {
        "id": "0UUrLuyTRYHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize correlation matrix with heatmap\n",
        "plt.figure(figsize = (14,7))\n",
        "sns.heatmap(corr_matrix, annot = True); #adding ';' to the end of this line removes the automatic annotation"
      ],
      "metadata": {
        "id": "2lLuym2FRlKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we can even correlate between specific feature sets using scatter matrix\n",
        "from pandas.plotting import scatter_matrix\n",
        "attribute_list = ['citric acid', 'pH', 'alcohol', 'sulphates', 'quality']\n",
        "scatter_matrix(exploration_set[attribute_list]);"
      ],
      "metadata": {
        "id": "qjDFu8oLSGyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 4: Prepare data for ML algorithm"
      ],
      "metadata": {
        "id": "zMVSDBCBUAmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Separate Features from Labels"
      ],
      "metadata": {
        "id": "MDn7fUoiDSQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#separate features and labels from the training set\n",
        "#copy all features leaving aside the labels\n",
        "wine_features = train_set_s.drop('quality', axis = 1)\n",
        "#copy the label list\n",
        "wine_labels = train_set_s['quality'].copy()\n",
        "#Data Cleaning\n",
        "#check for missing values in feature set\n",
        "wine_features.isna().sum()\n",
        "#if the count of NaN values in all feature is 0 then it means no missing data\n",
        "#if we have missing data we drop the rows containing them using dropna() method"
      ],
      "metadata": {
        "id": "HSUhJLfnT8Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Impute Missing Values"
      ],
      "metadata": {
        "id": "jxpM_HHpDWt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#impute missing values using median\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy = 'median')\n",
        "#if there is non numerical attributes they need to be dropped/categorized before imputing\n",
        "imputer.fit(wine_features)\n",
        "imputer.statistics_\n",
        "#statistics learnt by the imputer. the resulting array is a collection of median\n",
        "#values for each feature."
      ],
      "metadata": {
        "id": "2ctclkF8Ux6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we can cross check by directly calculating median\n",
        "wine_features.median()"
      ],
      "metadata": {
        "id": "885tS3RS_SYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#then we use the trained imputer to transform the training set such that missing\n",
        "#values are replaced by the medians\n",
        "tr_features = imputer.transform(wine_features)\n",
        "tr_features.shape\n",
        "#type(tr_features)"
      ],
      "metadata": {
        "id": "apmA5hRhLIKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now tr_features is an array which has just the numerical values in it.\n",
        "#we need to form it into a dataframe object again with proper column headings\n",
        "wine_features_tr = pd.DataFrame(tr_features, columns = wine_features.columns)\n",
        "wine_features_tr\n",
        "#type(wine_features_tr)"
      ],
      "metadata": {
        "id": "fsWFJsCnLr5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Handling Text and Categorical Attributes"
      ],
      "metadata": {
        "id": "tlARYhp4DcCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#converting categories to numbers\n",
        "#Method 1: Using OrdinalEncoder (for ordered (ordinal) data)\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "#then call fit_transform() method on ordinal_encoder object to convert text to\n",
        "#numbers\n",
        "#then the list of encoded categories can be viewed using categories_instance var\n",
        "\n",
        "#Note: One issue with this representation is that the ML algorithm would assume\n",
        "#that the two nearby values are closer than the distinct ones"
      ],
      "metadata": {
        "id": "UfZ2Bv8r_XW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Method 2: Using OneHotEncoder (for unordered (nominal) data)\n",
        "#here we create one binary feature per category: 1-present(hot) 0-absent(cold)\n",
        "#the new features are referred to as dummy features\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "cat_encoder = OneHotEncoder()\n",
        "#then call fit_transorm() on OneHotEncoder object\n",
        "#output is a SciPy sparse matrix rather than a numpy array. This saves space in\n",
        "#case of huge number of categories\n",
        "#in case we want the dense array format, convert it using toarray() method\n",
        "#the list of categories can be obtained via categories_instance variable\n",
        "\n",
        "#if number of category is huge, OHE will result in a large number of features.\n",
        "#address it by: 1) replacing them with categorical numerical features or,\n",
        "#2) convert to low dimensional learnable vectors called embeddings"
      ],
      "metadata": {
        "id": "ZB_PSvwf_rVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Feature Scaling and Transformation Pipeline"
      ],
      "metadata": {
        "id": "jepRk-ZyDmmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#most ML algos don't perform well when input features are on very different scales\n",
        "#scaling of target labels is generally not required\n",
        "\n",
        "#1) Min-max scaling or Normalization:\n",
        "#(Current value - Min value)/(Max value - Min value)\n",
        "#this way, all values fall in [0,1]\n",
        "#Scikit-Learn provides MinMaxScalar transformer for this\n",
        "#can specify hyperparameter feature_range to specify range of feature\n",
        "\n",
        "#2) Standardization:\n",
        "#(Current value - Mean value)/(Standard Deviation), to give a feature of unit variance\n",
        "#No bounds on data unlike Normalization\n",
        "#Less affected by outliers compared to Normalization\n",
        "#Scikit-Learn provides StandardScaler\n",
        "\n",
        "#Note: ALWAYS learn these transformers on training data and never on full data\n",
        "#only then apply them to training and test set to transform them\n",
        "\n",
        "#now we use pipeline to line up transformations in an intended order\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "transform_pipeline = Pipeline([('imputer', SimpleImputer(strategy = 'median')), \n",
        "                               ('std_scaler', StandardScaler()),])\n",
        "wine_features_tr = transform_pipeline.fit_transform(wine_features)\n",
        "#Missing value imputation followed by standardization\n",
        "#pairs of ('name',estimator) is defined for each step\n",
        "#__(double underscore) is not allowed in name"
      ],
      "metadata": {
        "id": "UPUy_m6aDof7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Transform Mixed features"
      ],
      "metadata": {
        "id": "U0HRwp6qhSYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#real world datasets have both categorical and numerical features and hence we\n",
        "#need to apply different transformations to them\n",
        "#Our dataset doesnt have Mixed Features but this would have been an ideal setup\n",
        "#if there were any, we use ColumnTransformer from Scikit-Learn\n",
        "\n",
        "#For illustration purpose, consider the example below:\n",
        "'''from sklearn.compose import ColumnTransformer\n",
        "num_attribs = list(wine_features)\n",
        "cat_attribs = [\"place_of_manufacturing\"]\n",
        "full_pipeline = ColumnTransformer([('num', num_pipeline, num_attribs), \n",
        "                                   ('cat', OneHotEncoder, cat_attribs)])\n",
        "wine_features_tr = full_pipeline.fit_transform(wine_features)'''\n",
        "\n",
        "#where num_pipeline is a pipeline which needs to be created to handle numerical\n",
        "#values, while OHE handles categorical\n",
        "#ColumnTransformer applies each transformation to the appropriate columns and then\n",
        "#concatenates the outputs along the columns\n",
        "#here both the transformation must return the same number of rows\n",
        "#we know the numerical transformation will return dense matrix while categorical\n",
        "#will return sparse. ColumnTransformer automatically determines type of output\n",
        "#base on density of resulting matrix"
      ],
      "metadata": {
        "id": "CuYD3Hytf3Km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 5: Select and train ML Models\n",
        "\n",
        "  * It is a good practice to build a quick baseline model on the preprocessed data and get an idea about model performance"
      ],
      "metadata": {
        "id": "3YKNQbasnjpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####a) Linear Regression"
      ],
      "metadata": {
        "id": "hniqfzMjfrpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(wine_features_tr, wine_labels)\n",
        "#Regression is now active. we can evaluate performance of model on training/test\n",
        "#sets. For regression we use MeanSquaredError as an evaluator\n",
        "from sklearn.metrics import mean_squared_error\n",
        "quality_predictions = lin_reg.predict(wine_features_tr)\n",
        "mean_squared_error(wine_labels, quality_predictions)"
      ],
      "metadata": {
        "id": "pEfrG1QAniwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets evaluate performance on a Test Set\n",
        "#copy all features apart from label\n",
        "wine_features_test = test_set_s.drop('quality', axis = 1)\n",
        "#copy label list\n",
        "wine_labels_test = test_set_s['quality'].copy()\n",
        "#apply transformation\n",
        "wine_features_test_tr = transform_pipeline.fit_transform(wine_features_test)\n",
        "#call predict function and perform MSE\n",
        "quality_test_predictions = lin_reg.predict(wine_features_test_tr)\n",
        "mean_squared_error(wine_labels_test, quality_test_predictions)"
      ],
      "metadata": {
        "id": "-_CnfWj7PfJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualising the error\n",
        "plt.scatter(wine_labels_test, quality_test_predictions)\n",
        "plt.plot(wine_labels_test, wine_labels_test, 'r-')\n",
        "plt.xlabel('Actual Quality')\n",
        "plt.ylabel('Predicted Quality');"
      ],
      "metadata": {
        "id": "yX_H58B1d91-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####b) Decision Tree Regressor"
      ],
      "metadata": {
        "id": "AyY0C2Rff0G_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model seems to be making error in the low/high quality regions\n",
        "#so we try Decision Tree Regressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "tree_reg = DecisionTreeRegressor()\n",
        "tree_reg.fit(wine_features_tr, wine_labels)\n",
        "#Evaluate performance of model on training set\n",
        "quality_predictions = tree_reg.predict(wine_features_tr)\n",
        "mean_squared_error(wine_labels, quality_predictions)"
      ],
      "metadata": {
        "id": "n4ZSNJORffcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training set has 0 error. Lets check the test set\n",
        "quality_test_predictions = tree_reg.predict(wine_features_test_tr)\n",
        "mean_squared_error(wine_labels_test, quality_test_predictions)\n",
        "#test error is 0.66 i.e. overfitted model"
      ],
      "metadata": {
        "id": "m_sGkbS5_2uS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(wine_labels_test, quality_test_predictions)\n",
        "plt.plot(wine_labels_test, wine_labels_test, 'r-')\n",
        "plt.xlabel('Actual Quality')\n",
        "plt.ylabel('Predicted Quality')"
      ],
      "metadata": {
        "id": "xCRrg8JFaIVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we can use cross validation (CV) for robust evaluation of model performance\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "#this provides a separate MSE for each validation set, which we can use to get a\n",
        "#mean estimation of MSE as well as the standard deviation, which helps us determine\n",
        "#how precise the estimate is\n",
        "#the additional cost for this step is additional training runs\n",
        "\n",
        "def display_scores(scores):\n",
        "  print(\"Scores: \", scores)\n",
        "  print(\"Mean: \", scores.mean())\n",
        "  print(\"Standard Deviation: \", scores.std())"
      ],
      "metadata": {
        "id": "Arrbzy-fzzQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Linear Regression CV"
      ],
      "metadata": {
        "id": "cuCl3yC80xNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = cross_val_score(lin_reg, wine_features_tr, wine_labels, \n",
        "                         scoring = \"neg_mean_squared_error\", cv = 10)\n",
        "lin_reg_mse_scores = -scores\n",
        "display_scores(lin_reg_mse_scores)"
      ],
      "metadata": {
        "id": "_RqwEpJx0woX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Decision Tree CV"
      ],
      "metadata": {
        "id": "2l_tUafh1VGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = cross_val_score(tree_reg, wine_features_tr, wine_labels, \n",
        "                         scoring = \"neg_mean_squared_error\", cv = 10)\n",
        "tree_mse_scores = -scores\n",
        "display_scores(tree_mse_scores)"
      ],
      "metadata": {
        "id": "viyMk-1O1OXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Random forest CV\n",
        "\n",
        "* It builds multiple decision trees on randomly selected features and then average their predections\n",
        "* Ensemble learning or building a model on top of another, improves performance of ML models"
      ],
      "metadata": {
        "id": "zQjbooBi1_xE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "forest_reg = RandomForestRegressor()\n",
        "forest_reg.fit(wine_features_tr, wine_labels)\n",
        "\n",
        "scores = cross_val_score(forest_reg, wine_features_tr, wine_labels, \n",
        "                         scoring = \"neg_mean_squared_error\", cv = 10)\n",
        "forest_mse_scores = -scores\n",
        "display_scores(forest_mse_scores)"
      ],
      "metadata": {
        "id": "5g7Q-Lrt2CU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quality_test_predictions = forest_reg.predict(wine_features_test_tr)\n",
        "mean_squared_error(wine_labels_test, quality_test_predictions)"
      ],
      "metadata": {
        "id": "DyMehXLr3Dg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(wine_labels_test, quality_test_predictions)\n",
        "plt.plot(wine_labels_test, wine_labels_test, 'r-')\n",
        "plt.xlabel('Actual quality')\n",
        "plt.ylabel('Predicted quality')"
      ],
      "metadata": {
        "id": "WEfoiVDA3hm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Random forest looks more promising than the previous two\n",
        "Note: Its a good practice to build a few such models quickly without tuning their hyperparameters and shortlist a few promising ones among them and save those models to disk in Python pickle format"
      ],
      "metadata": {
        "id": "xn6Aw0cf4Onq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 6: Finetune the model"
      ],
      "metadata": {
        "id": "ydQcveoy4z6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tuning hyperparameters lead to better accuracy of ML models\n",
        "#Scikit-Learn provides GridSearchSV for this purpose\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "XYXnRv8N34sw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}